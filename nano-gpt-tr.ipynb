{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  2 02:03:21 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.120                Driver Version: 537.58       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060        On  | 00000000:09:00.0  On |                  N/A |\n",
      "|  0%   51C    P8              16W / 170W |   1491MiB / 12288MiB |     32%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7eff6a118f50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.md', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 416180\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Acemi Ocağı\n",
      "\n",
      "Acemi Ocağı ya da Acemi Oğlanlar Ocağı, Osmanlı İmparatorluğu'nda Enderun için öğrencileri ve başta piyade kısmı olmak üzere Kapıkulu'nun ihtiyaç duyduğu askerleri yetiştirmek için kurulan ocaktır.\n",
      "\n",
      "Padişah I. Murad saltanatında Hayreddin Paşa ile Molla Rüstem'in girişimleriyle Gelibolu'da kurulan Acemi Ocağı'na ilk asker alımı, \"pençik\" adı verilen yöntemle savaşlarda elde edilen her beş erkek esirden birinin satın alınmasıyla gerçekleşti. Bu yöntem yerini zamanla devşirme sistemine bıraktı. Bu sisteme göre 3 ila 5 senede bir Osmanlı topraklarında yaşayan Hristiyanlardan 8 ila 20 yaş arasındaki çocuklar devşirilmeye başlandı. Devşirilenler İstanbul'a getirilip Müslüman yapılır ve sünnet edilirdi. Bir kısmı Bostancı Ocağı ve Enderun için ayrıldıktan sonra kalanlar 3 ila 5 yıl süreyle Türk ve İslam kaidelerini öğrenmeleri amacıyla Türk ailelerin yanına verilirdi. Buradaki görev sürelerini tamamlayanlar ulufe defterine kaydedilerek acemi oğlanı olmaya hak kazanırdı. Acemi \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 93\n",
      "['\\n', ' ', '\"', '#', '%', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ç', 'Ö', 'Ü', 'â', 'ç', 'ö', 'ü', 'ğ', 'İ', 'ı', 'Ş', 'ş']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Number of unique characters: {vocab_size}\")\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 59, 72, 62, 55, 56, 55, 9, 1, 68, 55, 73, 90, 66, 73, 90, 68, 28]\n",
      "Merhaba, nasılsın?\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "def encode(s): return [stoi[ch] for ch in s]\n",
    "def decode(a): return ''.join([itos[i] for i in a])\n",
    "\n",
    "\n",
    "print(encode(\"Merhaba, nasılsın?\"))\n",
    "print(decode(encode(\"Merhaba, nasılsın?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([416180]) torch.int64\n",
      "tensor([ 3,  1, 29, 57, 59, 67, 63,  1, 43, 57, 55, 88, 90,  0,  0, 29, 57, 59,\n",
      "        67, 63,  1, 43, 57, 55, 88, 90,  1, 79, 55,  1, 58, 55,  1, 29, 57, 59,\n",
      "        67, 63,  1, 43, 88, 66, 55, 68, 66, 55, 72,  1, 43, 57, 55, 88, 90,  9,\n",
      "         1, 43, 73, 67, 55, 68, 66, 90,  1, 89, 67, 70, 55, 72, 55, 74, 69, 72,\n",
      "        66, 75, 88, 75,  5, 68, 58, 55,  1, 33, 68, 58, 59, 72, 75, 68,  1, 63,\n",
      "        85, 63, 68,  1, 86, 88, 72, 59, 68, 57])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word: değerlendirilmemektedir 23 characters\n",
      "Average word length: 6.685192727811308 characters\n"
     ]
    }
   ],
   "source": [
    "def find_longest_word_in_turkish(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Initialize variables to store the longest word and its length\n",
    "    longest_word = \"\"\n",
    "    longest_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        # Remove punctuation and special characters from the word\n",
    "        cleaned_word = ''.join(e for e in word if e.isalnum() or e in 'ğĞıİöÖşŞçÇüÜ')\n",
    "\n",
    "        if len(cleaned_word) > longest_length:\n",
    "            longest_word = cleaned_word\n",
    "            longest_length = len(cleaned_word)\n",
    "\n",
    "    return longest_word\n",
    "\n",
    "\n",
    "def average_word_length(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    total_length = sum(len(word) for word in words)\n",
    "    num_words = len(words)\n",
    "\n",
    "    if num_words == 0:\n",
    "        return 0  # Avoid division by zero if there are no words in the text.\n",
    "\n",
    "    avg_length = total_length / num_words\n",
    "    return avg_length\n",
    "\n",
    "\n",
    "print(\"Longest word:\", find_longest_word_in_turkish(text), len(find_longest_word_in_turkish(text)), \"characters\")\n",
    "print(\"Average word length:\", average_word_length(text), \"characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[17, 20, 15,  1, 67, 55, 62, 55],\n",
      "        [ 1, 74, 55, 92, 90, 67, 55, 65],\n",
      "        [63, 55, 68, 74, 59, 70, 10, 39],\n",
      "        [68,  1, 58, 59,  1, 41, 59, 72]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[20, 15,  1, 67, 55, 62, 55, 66],\n",
      "        [74, 55, 92, 90, 67, 55, 65, 74],\n",
      "        [55, 68, 74, 59, 70, 10, 39, 55],\n",
      "        [ 1, 58, 59,  1, 41, 59, 72, 62]])\n",
      "\n",
      "when input is [17] the target: 20\n",
      "when input is [17, 20] the target: 15\n",
      "when input is [17, 20, 15] the target: 1\n",
      "when input is [17, 20, 15, 1] the target: 67\n",
      "when input is [17, 20, 15, 1, 67] the target: 55\n",
      "when input is [17, 20, 15, 1, 67, 55] the target: 62\n",
      "when input is [17, 20, 15, 1, 67, 55, 62] the target: 55\n",
      "when input is [17, 20, 15, 1, 67, 55, 62, 55] the target: 66\n",
      "when input is [1] the target: 74\n",
      "when input is [1, 74] the target: 55\n",
      "when input is [1, 74, 55] the target: 92\n",
      "when input is [1, 74, 55, 92] the target: 90\n",
      "when input is [1, 74, 55, 92, 90] the target: 67\n",
      "when input is [1, 74, 55, 92, 90, 67] the target: 55\n",
      "when input is [1, 74, 55, 92, 90, 67, 55] the target: 65\n",
      "when input is [1, 74, 55, 92, 90, 67, 55, 65] the target: 74\n",
      "when input is [63] the target: 55\n",
      "when input is [63, 55] the target: 68\n",
      "when input is [63, 55, 68] the target: 74\n",
      "when input is [63, 55, 68, 74] the target: 59\n",
      "when input is [63, 55, 68, 74, 59] the target: 70\n",
      "when input is [63, 55, 68, 74, 59, 70] the target: 10\n",
      "when input is [63, 55, 68, 74, 59, 70, 10] the target: 39\n",
      "when input is [63, 55, 68, 74, 59, 70, 10, 39] the target: 55\n",
      "when input is [68] the target: 1\n",
      "when input is [68, 1] the target: 58\n",
      "when input is [68, 1, 58] the target: 59\n",
      "when input is [68, 1, 58, 59] the target: 1\n",
      "when input is [68, 1, 58, 59, 1] the target: 41\n",
      "when input is [68, 1, 58, 59, 1, 41] the target: 59\n",
      "when input is [68, 1, 58, 59, 1, 41, 59] the target: 72\n",
      "when input is [68, 1, 58, 59, 1, 41, 59, 72] the target: 62\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # what is the maximum context length for predictions?\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"Targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print()\n",
    "\n",
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output: \n",
      "I%-%ÇJJ\n",
      "fs+wbjY,şşzIYWCIQy6IJ,oS4zRBr+qnŞOÜÜÜÖUÜ(UÖ>9#j0/xİ?:NIG)8c/ÖSGsÜpY5hhİeu#â\"QCIHOi/U+U.zIn39\n",
      "\n",
      "Logits Shape: torch.Size([32, 93])\n",
      "Loss: 5.089572429656982\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        # print(\"Logits:\", logits.shape)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(\"\\nOutput:\", decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "print(\"\\nLogits Shape:\", logits.shape)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.496530532836914\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10_000):  # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "\n",
      "\n",
      "11'nıkil dizmikacult yıkuptl Den, oleş -6üriliz setl yırtumaşti epışisiri dra 3 dirarişivnlatadeledasihakaşinm sınapı inası. ver vretıler te x\n",
      "I+r çtaru Tünllte ktıliyarluiğrdan kaka tisan 400.\n",
      "İbin uleriğıy; n. tiku Yakarokrbuki gantuvla bündia dem er yandüza yöpla hrl palın t ylgerileyaktaste yınditi yündınçicitame anırudöf eyldirenordalk aninbidarır sardillaktenölmatki. erı kahranünda desizelası. haylışt hen P,jUlınp kldağdışttı\n",
      "\n",
      "Alluth:rsetimivenişiibaklarüzırışerrirlerayıkağuradin ojkuzademaklınlük 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\", decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=512)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
